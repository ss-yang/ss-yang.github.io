<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[文档向量模型及其实践-计算文档的相似度]]></title>
    <url>%2F2017%2F06%2F25%2F%E6%96%87%E6%A1%A3%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%AE%9E%E8%B7%B5-%E8%AE%A1%E7%AE%97%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[期末大作业的其中一部分是要求对文档进行相似度计算，并提示可以用文档词向量的方法来做。于是查了一些资料。 然后引出了 空间向量模型（VSM） 这个概念。 空间向量模型 向量空间模型(VSM：Vector Space Model)由Salton等人于20世纪70年代提出，并成功地应用于著名的SMART文本检索系统。 VSM概念简单,把对文本内容的处理简化为向量空间中的向量运算,并且它以空间上的相似度表达语义的相似度,直观易懂。当文档被表示为文档空间的向量，就可以通过计算向量之间的相似性来度量文档间的相似性。 看完这段，顿悟了！不得不佩服向量的强大力量！ 既然知道了空间向量模型的存在，计算相似度就非常简单了！无非就是计算余弦值。 关键的地方是怎样构建文档向量？ 构建文档向量 原理&amp;思路： 要构建文档向量，应该选取最能代表这个文档的元素（特征值），很显然这个元素是文档中的关键词。（获取关键词的主要方法是分词） 关键词 就是 特征值 分词，计算出文档中关键词的词频。 然后，文档特征值应该有两个维度，一个是关键词本身，另一个是关键词出现的频数。（慢慢有了向量的感觉了） 当然，一个文档不止一个关键词，把所有n个关键词堆到一起，就得到了一个n * 2的矩阵。 最后，计算相似度的时候只用到频数，为了方便表示，取第二列转置得到向量b(v1,v2,…,vn)。这个n维向量就是表示该文档的向量，每一维度表示一个特征值，维度的长度表示特征值的频数。 相似性计算 步骤： 因为不同文档的特征值唯独可能不一样，而且相似性计算是相对于双方来说的，所以这里还要对上面的文档向量进一步构建（归一化，让维度相同）。 对要比较的文档的文档向量的关键词求全集。 分别将文档向量跟得到的全集比较，将对应关键词的频数填到全集的频数上，这样就得到了要拿来比较的两个文档的文档向量。每个向量都包含了对方的特征值维度，两个向量在相同的向量空间中。 根据向量的夹角余弦公式计算，夹角余弦值就是相似度。 计算结果说明： 第一组计算对象为2003年政府工作报告和2016年政府工作报告。 第二组计算对象为2003年政府工作报告和三体 I 阈值控制为10，词频低于阈值的将被忽略 这里事先计算好了两篇文章的词频。 计算结果： Report(2003) &amp; Report(2016) Similarity: 0.740816488615756 Process finished with exit code 0 Report(2003) &amp; TreeBody Similarity: 0.07395613415240768 Process finished with exit code 0 两篇政府工作报告的相似度是74%. 政府工作报告和三体的相似度是7%. 这个结果基本可以用来做文档分类了，要想得到更好的结果，应该优化分词的词典。 附上前10词频2016政府工作报告： 发展 92 推进 65 建设 63 创新 59 经济 48 改革 46 加快 44 加强 41 促进 40 实施 38 2003政府工作报告： 建设 78 发展 76 加强 63 坚持 54 对 48 实施 44 改革 43 积极 41 支持 38 我们 38 三体 I： 汪淼 623 中 521 叶文洁 433 三体 401 对 356 地 334 上 299 太阳 273 自己 271 文明 255 三体第一部出场最多的竟然不是叶文洁？？ 附上向量模型的源码： DocVector.class 12345678910111213141516171819202122232425262728293031323334353637package com.syang;import java.util.ArrayList;/** * Created by Answer on 2017/6/24. */public class DocVector &#123; private int dim; private ArrayList&lt;String&gt; keywords; private ArrayList&lt;Integer&gt; values; public int getDim() &#123; return dim; &#125; public void setDim(int dim) &#123; this.dim = dim; &#125; public ArrayList&lt;String&gt; getKeywords() &#123; return keywords; &#125; public void setKeywords(ArrayList&lt;String&gt; keywords) &#123; this.keywords = keywords; &#125; public ArrayList&lt;Integer&gt; getValues() &#123; return values; &#125; public void setValues(ArrayList&lt;Integer&gt; values) &#123; this.values = values; &#125;&#125; DocVecManager.class123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176package com.syang;import java.io.*;import java.util.Arrays;import java.util.ArrayList;import java.util.HashSet;import java.util.List;/** * Created by Answer on 2017/6/24. */public class DocVecManager &#123; private int THRESHOLD = 5; // 关键词阈值，频数低于这个值的关键词将被忽略 public void test()&#123; try &#123; DocVector wukong = parseFile2Vector(&quot;f:///分词词频-悟空传&quot;); DocVector report03 = parseFile2Vector(&quot;f:///分词词频-2003工作报告&quot;); DocVector report16 = parseFile2Vector(&quot;f:///分词词频-2016工作报告&quot;); DocVector santi = parseFile2Vector(&quot;f:///分词词频-三体I&quot;); DocVector[] dv = &#123;report16,report03&#125;; // 需要比较的的向量数组 DocVector merged = merge(dv); // 求向量的并集 List&lt;DocVector&gt; list = autoBuild(merged, dv); // 批量构建 System.out.println(&quot;Similarity: &quot; + calSimilarity(list.get(0),list.get(1))); &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; &#125; /** * 计算两个向量的夹角余弦 * @param built1 * @param built2 * @return */ public double calSimilarity(DocVector built1, DocVector built2)&#123; double multi = 0; //向量点乘 double temp1 = 0, temp2 = 0; // 两个向量的模 ArrayList&lt;Integer&gt; list1 = built1.getValues(); ArrayList&lt;Integer&gt; list2 = built2.getValues(); for (int i = 0; i &lt; built1.getDim(); i++)&#123; multi += list1.get(i) * list2.get(i); temp1 += list1.get(i) * list1.get(i); temp2 += list2.get(i) * list2.get(i); &#125; return multi / (Math.sqrt(temp1) * Math.sqrt(temp2)); &#125; /** * 自动构建多个文档向量 * @param merged * @param dv * @return */ public List&lt;DocVector&gt; autoBuild(DocVector merged, DocVector[] dv)&#123; List&lt;DocVector&gt; built = new ArrayList&lt;&gt;(); // 将dv中的每个向量跟并集build for(int i = 0; i &lt; dv.length; i++)&#123; built.add(buildVector(merged,dv[i])); &#125; return built; &#125; /** * 构建最终的文档向量模型 * 其结构为： * keywords为两个比较向量的keyword的全集，value为相应的value，如果不包含在全集中则为0. * @param merged 合并后的全集 * @param vector 需要构建的向量 * @return */ public DocVector buildVector(DocVector merged, DocVector vector)&#123; ArrayList&lt;String&gt; fullSet = merged.getKeywords(); // 获取合并后的keyword全集 Integer[] values = new Integer[merged.getDim()];// 将要赋给并集的value数组 Arrays.fill(values, 0); // 清零，确保不包含在全集中的keyword的value为0，避免出现null // 向全集中对应的keyword赋value for(int i = 0; i &lt; vector.getDim(); i++)&#123; int index = fullSet.indexOf(vector.getKeywords().get(i));// 在并集中查找vector的相应项 // 只要index有效，就代表存在，那么就把相应项的value写到并集中 if(index != -1)&#123; values[index] = vector.getValues().get(i); &#125; &#125; DocVector built = new DocVector(); built.setKeywords(fullSet); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(Arrays.asList(values)); // Integer[] values转换为ArrayList 并 set built.setValues(list); built.setDim(fullSet.size()); return built; &#125; /** * 求输入的多个DocVector的并集 * 合并多个DocVector的keywords，其values暂时为null。 * @param dv * @return */ public DocVector merge(DocVector[] dv) throws IllegalArgumentException&#123; DocVector merged = new DocVector(); int total = 0; ArrayList&lt;String&gt; k; ArrayList&lt;Integer&gt; v = new ArrayList&lt;&gt;(); k = dv[0].getKeywords(); // 将dv中所有向量的keyword累加到k中 for(int i = 1; i&lt;dv.length; i++)&#123; k = arrayListUnion(k,dv[i].getKeywords()); &#125; merged.setDim(k.size()); merged.setKeywords(k); merged.setValues(v); return merged; &#125; /** * 功能： * 根据path按行读取文件，并将每行分割为keyword和value，再根据阈值决定是否装入DocVector * @param path * @return DocVector * @throws IOException */ public DocVector parseFile2Vector(String path) throws IOException&#123; ArrayList&lt;String&gt; keywords = new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; values = new ArrayList&lt;&gt;(); File file = new File(path); // 获取文件句柄 InputStreamReader read = new InputStreamReader(new FileInputStream(file),&quot;utf-8&quot;); BufferedReader bufferedReader = new BufferedReader(read); String lineTxt = null; while((lineTxt = bufferedReader.readLine()) != null)&#123; String[] temp = lineTxt.split(&quot;\t| &quot;); // 正则表达式匹配空格，分隔一行 int t = Integer.parseInt(temp[1]); // 根据定义的阈值决定是否纳入 if(t &gt; THRESHOLD) &#123; keywords.add(temp[0]); values.add(t); &#125; &#125; read.close(); DocVector vector = new DocVector(); vector.setKeywords(keywords); vector.setValues(values); vector.setDim(keywords.size()); return vector; &#125; /** * 两个整数集求并集 * @param List1 * @param List2 * @return */ public &lt;T&gt;ArrayList&lt;T&gt; arrayListUnion( ArrayList&lt;T&gt; List1, ArrayList&lt;T&gt; List2) &#123; ArrayList&lt;T&gt; unionList = new ArrayList&lt;T&gt;(); unionList.addAll(List1); unionList.addAll(List2); unionList = new ArrayList&lt;T&gt;(new HashSet&lt;T&gt;(unionList)); return unionList; &#125; /** * 思路： * 1. read data * 2. parse to vector * 3. 根据阈值选择截断vector的前 N 个特征 * 4. 求出两个vector的全集 * 5. 在全集中加入相应vector的频数value（构建文档向量） * 6. 计算cosine */&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>中文分词</tag>
        <tag>向量模型</tag>
        <tag>特征值计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapRecuce 中文分词+词频统计+排序]]></title>
    <url>%2F2017%2F06%2F25%2FMapRecuce-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[词频统计和排序的思路&amp;步骤： SpliterMapper实现分词 分词可以借助其他分词工具，分词完成后，以key-value （word,1）形式wirte。 意思是把分词结果的每个词和其出现的次数（1次）以key-value的形式生成中间结果。 CounterReduce统计词频 统计mapper的结果，遍历values（values是一个Iteraor）,得到一个key的所有value，每个value的值都是1，把所有value相加，就得到了这个key的频数。 到这里词频统计就完成了。 排序 MR是以key为关键字排序的，而且默认升序，但是通常都需要降序的结果，这时只要继承Comparator 写一个类就可以了。 /** * 自定义IntWritableDecreasingComparator继承自IntWritable.Comparator * 因为Hadoop默认对IntWritable按升序排序 * 所以对compare取反，得到降序 */ private static class IntWritableDecreasingComparator extends IntWritable.Comparator { public int compare(WritableComparable a, WritableComparable b) { return -super.compare(a, b); } public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) { return -super.compare(b1, s1, l1, b2, s2, l2); } } 因为MR是以key为关键字排序，所以排序job的mapper使用hadoop提供的InverseMapper类把key和value反转 然后再job里面指定setSortComparatorClass为这个类。 至此，排序完成。 对排序结果的“美化” 因为上面排序的时候把key-value反转了，排序的结果是词频在前，词在后。 92 发展 65 推进 63 建设 59 创新 48 经济 46 改革 如果需要反过来…. 可以参考：hadoop MapReduce sort by value only 但是，，我走了几个弯路后发现，只要给sortJob重新写个sortRecucer就轻松解决问题了！ reduce sort结果的时候，把key和velue反写就ok。 public static class SortReducer extends Reducer&lt;IntWritable,Text,Text,IntWritable&gt;{ private Text result = new Text(); public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException{ for(Text val : values){ result.set(val); context.write(result, key); } } } 参考：hadoop中文分词、词频统计及排序 附上源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221package com.company;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;import org.apache.hadoop.mapreduce.lib.map.InverseMapper;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;import org.wltea.analyzer.core.IKSegmenter;import org.wltea.analyzer.core.Lexeme;import java.io.IOException;import java.io.StringReader;import java.text.DateFormat;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.Date;import java.util.Random;public class Main &#123; static final String HDFS_URL = &quot;hdfs://192.168.43.235:9000&quot;; static final String USER = &quot;hadoop&quot;; public static void main(String[] args) &#123;// spliterTest(); try &#123; HDFS hdfs = new HDFS(); hdfs.Init(HDFS_URL,USER);// hdfs.UpLoad(&quot;D://2017政府工作报告全文.txt&quot;, &quot;hdfs://192.168.43.235:9000/mapreduce/input/2017.txt&quot;); Date start = new Date(); run(&quot;hdfs://192.168.43.235:9000/mapreduce/input&quot;,&quot;/mapreduce/output&quot;); Date end = new Date(); float time = (float) ((end.getTime() - start.getTime()) / 60000.0); System.out.println(&quot;任务耗时：&quot; + String.valueOf(time) + &quot; 分钟&quot;); hdfs.Down(&quot;/mapreduce/output/part-r-00000&quot;,&quot;f://工作报告分词词频&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 分词器的测试 */ public static void spliterTest()&#123; String[] temp = IKAnalyzerSpliter(&quot;\n&quot; + &quot; 过去一年，我国发展面临国内外诸多矛盾叠加、风险隐患交汇的严峻挑战。在以习近平同志为核心的党中央坚强领导下，全国各族人民迎难而上，砥砺前行，推动经济社会持续健康发展。党的十八届六中全会正式明确习近平总书记的核心地位，体现了党和人民的根本利益，对保证党和国家兴旺发达、长治久安，具有十分重大而深远的意义。各地区、各部门不断增强政治意识、大局意识、核心意识、看齐意识，推动全面建成小康社会取得新的重要进展，全面深化改革迈出重大步伐，全面依法治国深入实施，全面从严治党纵深推进，全年经济社会发展主要目标任务圆满完成，“十三五”实现了良好开局。\n&quot;); // 不能识别英文句号 for(String s:temp)&#123; System.out.println(s); &#125; &#125; /** * IKAnalyzer分词器，返回String[]结果 * @param text * @return */ public static String[] IKAnalyzerSpliter(String text)&#123; ArrayList list = new ArrayList(); StringReader sr=new StringReader(text); IKSegmenter ik=new IKSegmenter(sr, true); Lexeme lex=null; try &#123; while ((lex = ik.next()) != null) &#123; list.add(lex.getLexemeText()); &#125; &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; return (String[])list.toArray(new String[list.size()]); &#125; /** * 清空缓存和输出文件夹，并运行Job * @param inputPath * @param outputPath * @throws Exception */ public static void run(String inputPath, String outputPath) throws Exception &#123; String tempPath = &quot;/mapreduce/count-temp-&quot; + new Random().nextInt(1000); // 临时存储统计结果 HDFS hdfs = new HDFS(); hdfs.Init(HDFS_URL,USER); if(counterJob(inputPath,tempPath)) &#123; sorterJob(tempPath,outputPath); &#125; hdfs.Del(tempPath); &#125; /** * 运行分词和统计的任务 * @param inputPath * @param outputPath * @return * @throws Exception */ public static boolean counterJob(String inputPath, String outputPath) throws Exception&#123; Job job = Job.getInstance(new Configuration()); job.setJarByClass(Main.class); job.setMapperClass(SpliterMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(inputPath)); job.setReducerClass(CounterReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); checkPath(outputPath); FileOutputFormat.setOutputPath(job, new Path(outputPath)); return job.waitForCompletion(true); &#125; /** * 运行对统计结果排序的任务 * @param outputPath * @param inputPath * @throws Exception */ public static void sorterJob(String inputPath, String outputPath) throws Exception&#123; Job sortJob = Job.getInstance(new Configuration()); sortJob.setJarByClass(Main.class); FileInputFormat.addInputPath(sortJob,new Path(inputPath)); sortJob.setInputFormatClass(SequenceFileInputFormat.class); sortJob.setMapperClass(InverseMapper.class); //实现map()之后的数据对的key和value交换 sortJob.setNumReduceTasks(1); // reducer个数 sortJob.setReducerClass(SortReducer.class); sortJob.setOutputKeyClass(IntWritable.class); sortJob.setOutputValueClass(Text.class); checkPath(outputPath); FileOutputFormat.setOutputPath(sortJob,new Path(outputPath)); sortJob.setSortComparatorClass(IntWritableDecreasingComparator.class); // 设置排序 sortJob.waitForCompletion(true); &#125; /** * 检查路径是否存在，如果存在就删除 * @param outputPath * @throws Exception */ private static void checkPath(String outputPath) throws Exception&#123; FileSystem fs = FileSystem.get(new Configuration()); Path output = new Path(outputPath); if(fs.exists(output))&#123; fs.delete(output,true); &#125; &#125; public static class SpliterMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123; public SpliterMapper()&#123;&#125; @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = IKAnalyzerSpliter(line); for (String w : words) &#123; context.write(new Text(w), new IntWritable(1)); &#125; &#125; &#125; public static class CounterReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; public CounterReducer()&#123;&#125; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int counter = 0; for (IntWritable l : values) &#123; counter += l.get(); &#125; context.write(key, new IntWritable(counter)); &#125; &#125; public static class SortReducer extends Reducer&lt;IntWritable,Text,Text,IntWritable&gt;&#123; private Text result = new Text(); public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException&#123; for(Text val : values)&#123; result.set(val); context.write(result, key); &#125; &#125; &#125; /** * 自定义IntWritableDecreasingComparator继承自IntWritable.Comparator * 因为Hadoop默认对IntWritable按升序排序 * 所以对compare取反，得到降序 */ private static class IntWritableDecreasingComparator extends IntWritable.Comparator &#123; public int compare(WritableComparable a, WritableComparable b) &#123; return -super.compare(a, b); &#125; public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return -super.compare(b1, s1, l1, b2, s2, l2); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
        <tag>中文分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习MapReduce遇到的一些小问题和解决办法记录]]></title>
    <url>%2F2017%2F06%2F25%2F%E5%AD%A6%E4%B9%A0MapReduce%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[说明：以下问题均在windows下连接Linux操作hadoop时出现，其中几个问题在Linux下面都不叫问题。 提交mapreduce作业的时候报错：找不到路径 Failed to locate the winutils binary in the hadoop binary path 解决方法参考：http://www.cnblogs.com/zq-inlook/p/4386216.html 下载winutils的windows版本https://github.com/srccodes/hadoop-common-2.2.0-bin 配置环境变量 增加用户变量HADOOP_HOME，值是下载的zip包解压的目录，然后在系统变量path里增加%HADOOP_HOME%\bin 即可。 提交mapreduce作业的时候报错：未定义的类 Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/util/Apps 解决方法： 从java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/util/Apps可以知道是Yarn的jar包没有找到。 于是，直接去linux上面把hadoop-2.7.3/share/hadoop/yarn文件夹里面的东西打包发到win上，再导入项目里面OK。 提交mapreduce作业的时候报错：没有写入权限 org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security .AccessControlException: Permission denied: user=Answer, access=WRITE, inode=&quot;hadoop&quot;: hadoop:supergroup:rwxr-xr-x 解决方法： 在hdfs上给输出文件夹加写入权限。 或者，在win的环境变量里加HADOOP_USER_NAME变量，值为linux上安装hadoop的用户名。 这里有详细的讲解：http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/ 提交mapreduce作业的时候报错：WCReducer$WCMappe没有init方法 java.lang.Exception: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.company.WCReducer$WCMapper.&lt;init&gt;() 解决方法： 参照stackoverflowd 方法在这两个类里面加上默认构造函数，也行不通。 最后直接把这两个类写成内部类就可以了。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper搭建过程简单记录]]></title>
    <url>%2F2017%2F06%2F25%2Fzookeeper%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在主节点配置文件到从节点用户目录下新建与主节点主机名同名的文件夹scp从主节点将配好的zookeeper发送到从节点上记得修改myid里面的序号（对应server.1,2,3…),记得改从节点的dataDir和dataLogDir的路径因为从节点的路径比主机到点少一级，这里忘记改路径导致启动一直报错（myid file is missing），找了好久都没有意识到是路径问题。。。掉进这个坑亏大了，注意细节！。 切记启动zookeeper集群要按照配置好的顺序来启动，前几台启动会警告拒绝连接，不用管，因为后面的节点还没启动。 启动了两个以上几点后，zookeeper根据自动选举leader,剩下的为follow，组成集群。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase安装过程简单记录]]></title>
    <url>%2F2017%2F06%2F25%2FHbase%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[解压改配置文件 把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf 把java环境变量加到hbase-env.sh export JAVA_HOME= /usr/java/jdk1.8.0_131 设置使用外部的zk export HBASE_MANAGES_ZK=false 最重要的，修改hbase-site.xml hbase.rootdir hdfs://yang:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum yang01:2181,yang02:2181 &lt;!—防止数据写入时发生权限问题–&gt; hbase.zookeeper.property.dataDir /zhangwei/zookeeper-3.4.5/zookeeper 修改regionservers，将里面的localhost删除，写上所有节点的hostname scp拷贝到其他节点同步时间启动！ 按顺序启动hadoop、zookeeper、hbase，其中一定要注意zk的启动顺序要遵循配置好的server的顺序。 成功！ 遇到的坑和解决方法 启动zookeeper失败 提示找不到其他节点。 解决方法：这不是问题，其他节点还没启动当然就找不到。只需要按顺序分别启动就可以了。 再有什么问题的话一定是配置文件的哪个参数不对，回去仔细检查！ 启动hbase 的时候抛异常 hbase java.net.UnknownHostException: ns1 原因：找不到主机名“ns1”,查资料后得知，我很尴尬的把这个主机名当文件夹了23333 解决方法：把hbase-site.xml里相应的值对应core-site.xml改过来就可以了。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android 让TextView的文字支持点击跳转（类似超链接）]]></title>
    <url>%2F2017%2F05%2F16%2FAndroid-%E8%AE%A9TextView%E7%9A%84%E6%96%87%E5%AD%97%E6%94%AF%E6%8C%81%E7%82%B9%E5%87%BB%E8%B7%B3%E8%BD%AC%EF%BC%88%E7%B1%BB%E4%BC%BC%E8%B6%85%E9%93%BE%E6%8E%A5%EF%BC%89%2F</url>
    <content type="text"><![CDATA[TextView本身不支持onClick事件，不过借助ClickableSpan类的onClick和SpannableString类可以很方便地实现跳转： TextView login_quit = (TextView)headerView.findViewById(R.id.tv_login_quit); String login = &quot;登陆&quot;; SpannableString spannableString=new SpannableString(login); spannableString.setSpan(new ClickableSpan() { @Override public void onClick(View view) { Intent intent=new Intent(MainActivity.this,LoginActivity.class); startActivity(intent); } }, 0, login.length(), Spanned.SPAN_EXCLUSIVE_EXCLUSIVE); login_quit.setText(spannableString); login_quit.setMovementMethod(LinkMovementMethod.getInstance()); 这样就可以实现点击文字跳转的功能了。 附上setSpan函数的原型，理解各个参数： public void setSpan(Object what, int start, int end, int flags) { super.setSpan(what, start, end, flags); } 效果：]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Material Design 获取NavigationView的headerView的方法]]></title>
    <url>%2F2017%2F05%2F16%2FAndroid-Material-Design-%E8%8E%B7%E5%8F%96NavigationView%E7%9A%84headerView%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[headerView NavigationView的headerView通常用来放用户信息。 找不到headerView的原因 今天在这里实现点击“登陆”跳转功能的时候踩了个小坑，一直闪退，debug后才发现在这一句出问题（原来一直怀疑是ClickableSpan的问题，从一开始就走错了方向，耗了不少时间）： TextView login = (TextView)findViewById(R.id.tv_login_quit); 这里login的值为null，终于找到原因：findViewById找不到布局文件里面的TextView控件。 仔细看就发现headerView是一个子布局，父布局是NavigationView，那么问题就解决了！ 获取headerView的正确姿势 先获取到NavigationView NavigationView navigationView = (NavigationView) findViewById(R.id.nav_view); 再通过NavigationView获取headerView View headerView = navigationView.getHeaderView(0); 其中getHeaderView(int index) 传入0就可一，index是nav中header的序号，一般只有一个header. - 最后，通过headerView获取headerView里面的控件 TextView login = (TextView)headerView.findViewById(R.id.tv_login_quit); 注意是****headerView.findViewById****]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>MaterialDesign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android-用Intent在活动之间传递数据的三种方法]]></title>
    <url>%2F2017%2F05%2F14%2FAndroid-%E7%94%A8Intent%E5%9C%A8%E6%B4%BB%E5%8A%A8%E4%B9%8B%E9%97%B4%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Intent intent n. 意图；目的； Intent最常见的用法是启动活动、服务等，实现Activity间跳转。 那么问题来了，在跳转的时候，怎样给下一个Activity捎一些数据呢？ 三种方法 putExtra serializable parcelable - putExtra()Intent类提供了putExtra()方法，可以向intent对象添加一些常见类型的附加数据。只要在startActivity之前调用即可。 intent.putExtra(&quot;string_data&quot;,&quot;hello&quot;); 然后在另一边用getStringExtra()接收： getIntent.getStringExtra(&quot;string_data&quot;); 这种方法很简单，简单到只能传基本数据类型的数据，要传自定义的对象就无能为力了。 - serializable这是一个接口，serializable即序列化，能将对象转化成可以直接传输的数据，可以理解为将对象简单粗暴的用一串二进制表示，然后就可传输了。 用法非常简单，只要在要传输的类定义里面实现Serializable接口。 public class Article implements Serializable{ …… } putExtra()跟上面的方法一样，只是把put的对象换成自定义的对象了 intent.putExtra(&quot;article&quot;,article); 再到下一个Activity取出传过来的序列化对象，就完成了。 article = (Article) getIntent.getSerializbleExtra(&quot;article&quot;); 是不是超级简单粗暴！ 但由于是直接传递整个对象，不可避免的会降低效率，尤其是传递体量较大的对象时。这个时候推荐用parcelable方法来实现。 - parcelableparcelable即包裹化，原理是先将对象拆开，再把真正有用的数据打包带走。 相比序列化的方法，parcelable实现了精准传递，很多时候我只需要对象的某些数据，并不是所有数据。也因为这种机制，这种方式实现起来较为复杂（拆开和打包需要自己动手）。 1. 实现接口public class Article implements Parcelable{ private int id; private String title; private String author; private String content; private String time; private String last_time; 2. 先看看源码（Parcelable.java）public interface Parcelable { public static final int PARCELABLE_WRITE_RETURN_VALUE = 0x0001; public static final int PARCELABLE_ELIDE_DUPLICATES = 0x0002; public static final int CONTENTS_FILE_DESCRIPTOR = 0x0001; public int describeContents(); public void writeToParcel(Parcel dest, int flags); public interface Creator&lt;T&gt; { public T createFromParcel(Parcel source); public T[] newArray(int size); } public interface ClassLoaderCreator&lt;T&gt; extends Creator&lt;T&gt; { public T createFromParcel(Parcel source, ClassLoader loader); } } 这里只需要重写describeContents、writeToParcel和Creator 3. 重写describeContents通常，这里什么都不用做，直接返回0. @Override public int describeContents(){ return 0; } 4. 重写writeToParcel通过writeXXX()方法写出对象的属性。XXX可以是各种受支持的数据类型。 @Override public void writeToParcel(Parcel dest, int flag){ dest.writeInt(id); dest.writeString(title); dest.writeString(author); dest.writeString(content); dest.writeString(time); dest.writeString(last_time); } 5. Creator从源码可以看出，public interface Creator 是一个泛型接口，里面需要重写两个方法。 createFromParcel方法将打包的数据读取，重新封装成泛型指示的对象。 newArray方法返回一个对象数组。 public static final Parcelable.Creator&lt;Article&gt; CREATOR = new Creator&lt;Article&gt;(){ @Override public Article createFromParcel(Parcel source){ Article article = new Article(); article.setId(source.readInt()); article.setTitle(source.readString()); article.setAuthor(source.readString()); article.setContent(source.readString()); article.setTime(source.readString()); article.setLast_time(source.readString()); return article; } @Override public Article[] newArray(int size){ return new Article[size]; } }; 注意这里读取数据的顺序应该和上一步写数据的顺序一样。 6. 在下一个活动中接收数据接受数据和前两种方法相差无几。用的是getParcelableExtra()方法。 总结三种传数据的方法的思路都是一样的，都是A把数据put给intent，B从intent里面读取。不同的是实现的细节，第一种只能put一些基本数据类型，后两种方法进行了扩展，可以支持自定义类型。其中serializable直接将序列化后的对象传递，parcelable将对象的属性写入包裹，再从包裹重建对象，比前者效率高，但实现起来比前者复杂。 附：Intent putExtra支持的所有类型 Intent putExtra(String name, String[] value) Intent putExtra(String name, long value) Intent putExtra(String name, boolean value) Intent putExtra(String name, double value) Intent putExtra(String name, Parcelable[] value) Intent putExtra(String name, char value) Intent putExtra(String name, int[] value) Intent putExtra(String name, int value) Intent putExtra(String name, double[] value) Intent putExtra(String name, short value) Intent putExtra(String name, long[] value) Intent putExtra(String name, boolean[] value) Intent putExtra(String name, short[] value) Intent putExtra(String name, String value) Intent putExtra(String name, float[] value) Intent putExtra(String name, Bundle value) Intent putExtra(String name, byte[] value) Intent putExtra(String name, CharSequence value) Intent putExtra(String name, char[] value) Intent putExtra(String name, byte value) Intent putExtras(Intent src) Intent putExtras(Bundle extras) Intent putExtra(String name, Serializable value) Intent putExtra(String name, Parcelable value)]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android-RecyclerView通过addOnItemTouchListener处理onClick点击事件]]></title>
    <url>%2F2017%2F05%2F13%2FAndroid-RecyclerView%E9%80%9A%E8%BF%87addOnItemTouchListener%E5%A4%84%E7%90%86onClick%E7%82%B9%E5%87%BB%E4%BA%8B%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[原因现在android开发中ListView效果越来越多的用RecyclerView来实现了，RecyclerView相比前者有非常多的优点，还可以轻松实现瀑布流布局、扇形列表等。。。 但是！！ 用的时候发现RecyclerView没有提供OnItemClick()，网上查的大多数博客都采用在Adapter里面自己定义接口的方法来模拟ListView的点击事件,有些还模拟了长按、短按等功能。 解决这样模拟终究不是好方法，官方虽然没有提供click方法，但是提供了更为强大的addOnItemTouchListener接口，结合GestureDetectorCompat手势检测可以方便的实现很多功能。 1. 为recycleView添加监听器recyclerView.addOnItemTouchListener(new RecyclerView.OnItemTouchListener() { @Override public boolean onInterceptTouchEvent(RecyclerView rv, MotionEvent e) { return false; } @Override public void onTouchEvent(RecyclerView rv, MotionEvent e) { } @Override public void onRequestDisallowInterceptTouchEvent(boolean disallowIntercept) { } }); 其中： onInterceptTouchEvent：拦截触摸事件 onTouchEvent：处理触摸事件 onRequestDisallowInterceptTouchEvent：通常用于请求ViewPager不要拦截该控件上的触摸事件。 很明显，现在需要拦截触摸事件，拦截下来才能实现功能 2. 既然只要处理onInterceptTouchEvent，那就可以删掉另外两个方法。3. 声明一个GestureDetector，传入GestureDetector.SimpleOnGestureListener实例化。SimpleOnGestureListener提供的onSingleTapUp和onLongPress，正是想要的功能！！ GestureDetector gestureDetector; gestureDetector = new GestureDetector(this, new GestureDetector.SimpleOnGestureListener(){ @Override public boolean onSingleTapUp(MotionEvent e){ View childView = recyclerView.findChildViewUnder(e.getX(), e.getY()); if (childView != null) { int position = recyclerView.getChildLayoutPosition(childView); Toast.makeText(getApplication(), &quot;single click:&quot; + position, Toast.LENGTH_SHORT).show(); return true; } return super.onSingleTapUp(e); } @Override public void onLongPress(MotionEvent e) { super.onLongPress(e); View childView = recyclerView.findChildViewUnder(e.getX(), e.getY()); if (childView != null) { int position = recyclerView.getChildLayoutPosition(childView); Toast.makeText(getApplication(), &quot;long click:&quot; + position, Toast.LENGTH_SHORT).show(); } } }); 这里不能直接声明为SimpleOnGestureListener，因为要用到父类的onTouchEvent 4. 最后，在第1步的onInterceptTouchEvent里面加上一个简单的判断就可以了。@Override public boolean onInterceptTouchEvent(RecyclerView rv, MotionEvent e) { if (gestureDetector.onTouchEvent(e)) { return true; } return false; } 5. 完成！ 进阶：更完善的解决方法 - 封装成接口，方便重用 先说结果，封装成接口后，调用代码从上面一大堆变成短短几行，清晰明了。 recyclerView.addOnItemTouchListener(new RecyclerItemClickListener(getBaseContext(), recyclerView, new RecyclerItemClickListener.OnItemClickListener() { @Override public void onItemClick(View view, int position) { //轻触... } @Override public void onItemLongClick(View view, int position) { //长按... } })); 只需要写回调函数里的逻辑即可。 怎么封装类呢？ 1. 新建一个类 RecyclerItemClickListener，包含一个Item点击监听器和一个手势检测：public class RecyclerItemClickListener implements RecyclerView.OnItemTouchListener { private OnItemClickListener mListener; private GestureDetector gestureDetector; } 2. 定义一个OnItemClickListener接口，里面包含两个回调函数public interface OnItemClickListener { void onItemClick(View view, int position); void onItemLongClick(View view, int position); } 3. 构造函数构造函数需要接收Context、RecyclerView、OnItemClickListener三个参数. Context 上下文传递 RecyclerView 接口实现的功能跟RecyclerView密切相关，可以通过这个参数获取position OnItemClickListener 可以理解为回调函数的通道 public RecyclerItemClickListener(Context context, final RecyclerView recyclerView, OnItemClickListener listener) { mListener = listener; gestureDetector = new GestureDetector(context, new GestureDetector.SimpleOnGestureListener() { @Override public boolean onSingleTapUp(MotionEvent e) { View child = recyclerView.findChildViewUnder(e.getX(), e.getY()); if (child != null &amp;&amp; mListener != null) { mListener.onItemClick(child, recyclerView.getChildAdapterPosition(child)); } return true; } @Override public void onLongPress(MotionEvent e) { View child = recyclerView.findChildViewUnder(e.getX(), e.getY()); if (child != null &amp;&amp; mListener != null) { mListener.onItemLongClick(child, recyclerView.getChildAdapterPosition(child)); } } }); } 4. 重写onInterceptTouchEvent()@Override public boolean onInterceptTouchEvent(RecyclerView view, MotionEvent e) { View childView = view.findChildViewUnder(e.getX(), e.getY()); if (childView != null &amp;&amp; mListener != null &amp;&amp; gestureDetector.onTouchEvent(e)) { mListener.onItemClick(childView, view.getChildAdapterPosition(childView)); return true; } return false; } 5. 完成 参考资料: android-RecyclerView onClick-Stack Overflow]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop - CentOS7搭建Hadoop集群（二）]]></title>
    <url>%2F2017%2F05%2F12%2FHadoop-CentOS7%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hadoop - CentOS7搭建Hadoop集群（一） 3. 在master上安装Hadoop 用putty、SecureCRT、XShell等工具把hadoop2.7.3.tar.gz（从windows）上传到master上，然后解压到用户目录里。 打开 /home/hadoop/hadoop2.7.3/etc/hadoop/目录，修改hadoop的配置文件： hadoop-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_131 yarn-env.sh export JAVA_HOME=/usr/java/jdk1.8.0_131 core-site.xml &lt;configuration&gt; &lt;!-- 指定 HDFS（namenode）的通信地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://yang:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定 hadoop 运行时产生文件的存储路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop-2.7.3/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml &lt;configuration&gt; &lt;!-- 设置 hdfs 副本数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml.template 需要重命名： mv mapred-site.xml.template mapred-site.xml &lt;configuration&gt; &lt;!-- 通知框架 MR 使用 YARN --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;yang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; 格式化 hdfs namenode -format 4. 安装Hadoop集群摘要： 把在master配置好的hadoop直接打包复制到每个slave上，修改主机名，格式化每个slave，到master的slaves文件指定slave节点的主机名，启动集群。 把在master配置好的hadoop打包，用scp传到其他的slave上。 scp scp hadoop.tar.gz hadoop@192.168.5.152:/home/hadoop 修改每个slave的 hadoop配置文件的主机名（参照master中几个配置文件）。 为每个slave节点配置静态ip。 为了能通过主机名直接访问节点，还需要修改/etc/hosts文件，为每个节点添加ip地址映射。这里直接在一个节点上添加集群所有节点的ip映射，然后copy到其他每个节点。测试一下，ok。 回到master，找到hadoop2.7.3/etc/hadoop/slaves文件，加入所有slaves的主机名。 vi hadoop2.7.3/etc/hadoop/slaves slave1 slave2 ~ ~ 注：slaves文件默认内容为localhost，需要先将这个删除，再添加slave的主机名。 切换到hadoop2.7.3/sbing目录下，执行start-all.sh脚本启动集群。 用jps命令查看运行状态。 至此，安装全部完成。 5. 需要注意的的一些坑 master和slave的hadoop建议统一安装在相同的目录下，事先做好规定很重要（确定安装目录，确定用户名，确定ip，确定主机名）。 集群的每个节点都要用同名用户(hadoop)。 确保hadoop2.7.3文件夹里面的所有文件的权限都属于hadoop用户，而不是root，否则访问某些文件没有权限导致格式化失败or系统启动失败。 修改slaves文件的时候，要把默认的localhost删除，因为localhost不是slave节点。 配置ssh免密码登陆的时候，authorized_keys的权限应该是“-rw——-”，否则master没有权限访问slave的这个文件，就不能免密登陆。 Hadoop - CentOS7搭建Hadoop集群（一）]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop - CentOS7搭建Hadoop集群（一）]]></title>
    <url>%2F2017%2F05%2F11%2FHadoop-CentOS7%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[环境： CentOS-7-x86_64-Minimal hadoop2.7.3a jdk1.8.0_131 1. linux安装1. 安装虚拟机软件（VirtualBox和VMWare都可以）2. 新建centos虚拟机，挂载centos7镜像3. 选择最小安装+开发工具 2. 安装Hadoop前的准备安装hadoop可以用root用户也可以新建一个用户,建议新建一个用户。 1. 把ip地址改为静态ip，以免以后要改来改去修改网络连接配置文件 vi /etc/sysconfig/network-scripts/ifcfg-eth0 ... BOOTPRORO=static IPADDR=ip地址 ... 2. 设置DNSvi /etc/resolv.conf nameserver 114.114.114.114 3. 测试一下网络是不是正常4. 修改主机名vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=yourname :wq 5. 关防火墙systemctl stop firewalld.service systemctl disable firewalld.service firewall-cmd --state #查状态 6. 配置ssh免密码登陆在每台服务器上生成公钥密钥，再将公钥合并到master的authorized_keys文件上。 注意！！master和slave都要用相同的user和group (这里为hadoop) centos默认关闭ssh免密码登陆，需要到/etc/ssh/sshd_config文件把下面两行取消注释： RSAAuthentication yes PubkeyAuthentication yes 然后，在master和每个slave上分别生成密钥： ssh-keygen -t rsa 将slave的公钥合并到authorized_keys文件，在master中进入/home/hadoop/.ssh目录，用重定向符号合并： 123cat id_rsa.pub&gt;&gt; authorized_keysssh hadoop@192.168.0.183 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keysssh hadoop@192.168.0.184 cat ~/.ssh/id_rsa.pub&gt;&gt; authorized_keys 合并完成后，再将master的authorized_keys和known_hosts文件通过scp命令复制到每个slave的.ssh目录里，至此，每台slave和master都互有公钥。 12scp /home/hadoop/.ssh/known_hosts hadoop@192.168.5.134:/home/hadoop/.ssh/scp /home/hadoop/.ssh/authorized_keys hadoop@192.168.5.134:/home/hadoop/.ssh/ 回master测试，已经可以无需密码登陆其他slave了(第一次需要密码)，如果失败，看6。 [hadoop@yang .ssh]$ ssh hadoop@192.168.5.132 Last login: Thu May 11 20:51:47 2017 from 192.168.5.133 [hadoop@yang01 ~]$ 如果免密码登陆失败，到slave把authorized_keys的权限改成“-rw——-”就可以了： chmod 600 authorized_keys [hadoop@yang01 .ssh]$ ll -rw-------. 1 hadoop hadoop 1183 5月 11 20:18 authorized_keys 7. 安装jdk 到官网下载最新jkd版本的rpm包 rpm -ivh jdk**.rpm 配置环境变量 java -version 测试 Hadoop - CentOS7搭建Hadoop集群（二）]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android - ToolBar searchView 实现搜索框]]></title>
    <url>%2F2017%2F05%2F02%2FAndroid-ToolBar-searchView-%E5%AE%9E%E7%8E%B0%E6%90%9C%E7%B4%A2%E6%A1%86%2F</url>
    <content type="text"><![CDATA[在ToolBar上可以很方便的用widget.SearchView实现搜索功能。一般情况下，SearchView通常有两种实现方案： 在当前Activity处理搜索逻辑 首先在menu中新增item 12345&lt;item android:id=&quot;@+id/toolbar_search&quot; android:title=&quot;Search&quot; app:actionViewClass=&quot;android.support.v7.widget.SearchView&quot; app:showAsAction=&quot;always&quot;/&gt; onCreateOptionsMenu中增加如下代码： 1234567891011121314151617181920212223 public boolean onCreateOptionsMenu(Menu menu)&#123; getMenuInflater().inflate(R.menu.tool_bar,menu); //Toolbar的搜索框 MenuItem searchItem = menu.findItem(R.id.toolbar_search); SearchView searchView = null; if (searchItem != null) &#123; searchView = (SearchView) searchItem.getActionView(); &#125; searchView.setOnQueryTextListener(new SearchView.OnQueryTextListener() &#123; @Override public boolean onQueryTextSubmit(String query) &#123; //处理搜索结果 Toast.makeText(MainActivity.this,&quot;搜索: &quot; + query,Toast.LENGTH_LONG).show(); return false; &#125; @Override public boolean onQueryTextChange(String s) &#123; return false; &#125; &#125;); return true;&#125; 在新Activity处理搜索逻辑 在menu中新增item（同上） 在onCreateOptionsMenu中增加如下代码： 123456789101112131415public boolean onCreateOptionsMenu(Menu menu) &#123; MenuInflater menuInflater = getMenuInflater(); menuInflater.inflate(R.menu.dashboard, menu); MenuItem searchItem = menu.findItem(R.id.action_search); SearchManager searchManager = (SearchManager) MainActivity.this.getSystemService(Context.SEARCH_SERVICE); SearchView searchView = null; if (searchItem != null) &#123; searchView = (SearchView) searchItem.getActionView(); &#125; if (searchView != null) &#123; searchView.setSearchableInfo(searchManager.getSearchableInfo(MainActivity.this.getComponentName())); &#125; return super.onCreateOptionsMenu(menu);&#125; 在res/xml/下新建searchable.xml文件，内容如下： 1234&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;searchable xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;android:hint=&quot;@string/search_hint&quot;android:label=&quot;@string/app_name&quot; /&gt; 新建处理搜索结果的activity：SearchResultsActivity AndroidManifest文件添加下面的代码： 1234567891011121314&lt;activity android:name=&quot;com.example.SearchResultsActivity&quot; android:label=&quot;@string/app_name&quot;&gt; &lt;intent-filter&gt; &lt;action android:name=&quot;android.intent.action.SEARCH&quot; /&gt; &lt;/intent-filter&gt; &lt;intent-filter&gt; &lt;action android:name=&quot;android.intent.action.VIEW&quot; /&gt; &lt;/intent-filter&gt; &lt;meta-data android:name=&quot;android.app.searchable&quot; android:resource=&quot;@xml/searchable&quot; /&gt;&lt;/activity&gt; 在SearchResultsActivity里面处理搜索结果]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7 创建应用程序快捷启动器的方法]]></title>
    <url>%2F2017%2F04%2F30%2Fcentos7-%E5%88%9B%E5%BB%BA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%BF%AB%E6%8D%B7%E5%90%AF%E5%8A%A8%E5%99%A8%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原因在linux下装好了android studio, 却发现只能在安装目录下通过terminal执行studio.sh来运行，这样明显很不方便。但作为一个linux新司机，并不知道怎么办。桌面右建-&gt;新建启动器，centos里根本没有这一项。后来因为某些原因需要重装中国版的firefox，官方安装包下载下来后，也是直接解压使用的(需要从terminal启动）。同一个问题遇到两次，不能忍。 解决方法查资料后得知，centos 的程序图表都在/usr/share/applications/文件夹下，里面都是.desktop文件。于是问题迎刃而解。 新建一个android-studio.desktop文件，内容如下： [Desktop Entry] Encoding=UTF-8 Name=Android Studio 2.3 Comment=Android Studio Comment[zh_CN]=安卓开发 Exec=/usr/local/bin/android-studio #运行路径 Icon=/opt/android-studio/bin/studio.png #图标 Categories=Application;Development;Android; #分类：这个将在应用的编程分类中 Actions=NewWindow;NewPrivateWindow; Type=Application Terminal=0 #是否命令行形式 至此，应用程序菜单里面就有了刚刚添加的启动器了，也可以将desktop文件复制到桌面。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>